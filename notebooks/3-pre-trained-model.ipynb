{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline output:\n",
      " [{'label': 'POSITIVE', 'score': 0.999874472618103}, {'label': 'NEGATIVE', 'score': 0.9996241331100464}]\n"
     ]
    }
   ],
   "source": [
    "# 1) Install / import\n",
    "!pip install transformers        # run once in Colab or your venv\n",
    "from transformers import pipeline, DistilBertTokenizer\n",
    "\n",
    "# 2) ‚Äî‚Äî Example A: use the ü§ó pipeline for sentiment analysis ‚Äî‚Äî\n",
    "# Here we explicitly pick a model fine‚Äêtuned on SST-2 (binary sentiment).\n",
    "sentiment_pipe = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    ")\n",
    "\n",
    "# Pass a list of texts and get back labels + scores\n",
    "samples = [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"I really hated how it dragged on and on‚Ä¶\",\n",
    "]\n",
    "preds = sentiment_pipe(samples)\n",
    "print(\"pipeline output:\\n\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original sentence:\n",
      "Tokenize this sentence using DistilBERT.\n",
      "\n",
      "Input IDs:\n",
      "[101, 19204, 4697, 2023, 6251, 2478, 4487, 16643, 23373, 1012, 102]\n",
      "\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Decoded tokens:\n",
      "   101 ‚Üí [CLS]\n",
      " 19204 ‚Üí token\n",
      "  4697 ‚Üí ##ize\n",
      "  2023 ‚Üí this\n",
      "  6251 ‚Üí sentence\n",
      "  2478 ‚Üí using\n",
      "  4487 ‚Üí di\n",
      " 16643 ‚Üí ##sti\n",
      " 23373 ‚Üí ##lbert\n",
      "  1012 ‚Üí .\n",
      "   102 ‚Üí [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 3) ‚Äî‚Äî Example B: raw DistilBERT tokenization + introspection ‚Äî‚Äî\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "sentence = \"Tokenize this sentence using DistilBERT.\"\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# pull out IDs & attention mask\n",
    "input_ids     = tokens[\"input_ids\"].squeeze().tolist()\n",
    "attention_mask = tokens[\"attention_mask\"].squeeze().tolist()\n",
    "\n",
    "print(f\"\\nOriginal sentence:\\n{sentence}\\n\")\n",
    "print(f\"Input IDs:\\n{input_ids}\\n\")\n",
    "print(f\"Attention mask:\\n{attention_mask}\\n\")\n",
    "\n",
    "# map each ID back to its token\n",
    "print(\"Decoded tokens:\")\n",
    "for idx in input_ids:\n",
    "    print(f\"{idx:6d} ‚Üí {tokenizer.decode([idx])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
